[TOC]

**让相似样本靠近、让不相似样本远离** 的一种损失函数，用于学“表示/嵌入”。

## 一、最经典的“成对”对比损失（Siamese 那个）

先从最传统、最容易写成公式的版本说起 —— Hadsell 等人在 Siamese 网络里用的那种 **成对对比损失**。

### 1.1 场景设定

我们有一对样本 $(x_1, x_2)$，以及一个标签：

- $y = 1$：表示 **正样本对**（similar / same class / 同一人脸）
- $y = 0$：表示 **负样本对**（dissimilar / 不同类 / 不同人）

网络 $f(\cdot)$ 把输入映射到某个向量空间（嵌入/特征）：
$$
z_1 = f(x_1),\quad z_2 = f(x_2)
$$
接着定义它们的距离（一般用欧氏距离）：
$$
D = \| z_1 - z_2 \|_2
$$

### 1.2 损失函数公式

一个经典形式是（带 margin 的）：
$$
L = y \cdot D^2 + (1 - y) \cdot \max(0, m - D)^2
$$
其中：

- $y \in \{0, 1\}$
- $m > 0$ 是一个 **margin（间隔）超参数**

- 当 y=1（即样本相似）时，损失函数只剩下 $$∑yd2$$ ，即原本相似的样本，如果在特征空间的欧式距离较大，则说明当前的模型不好，因此加大损失。
- 当 y=0 时（即样本不相似）时，损失函数为 $$∑(1−y)max(margin−d,0)2$$ ，即当样本不相似时，其特征空间的欧式距离反而小的话，损失值会变大，这也正好符号我们的要求。

### 1.3 这公式在干嘛？

分两种情况看：

#### ✅ 情况 1：正样本对（y = 1）

$$
L_{\text{pos}} = D^2
$$

- 希望 $D$ 越小越好，也就是 $z_1, z_2$ 距离越来越近
- 训练会推动网络让相似样本嵌入向量 **靠在一起**

直觉：

> 同一类 / 同一个人的图片，在特征空间里要“挤在一块”。

------

#### ✅ 情况 2：负样本对（y = 0）

$$
L_{\text{neg}} = \max(0, m - D)^2
$$

- 如果 **D 已经 ≥ m**：
   → $\max(0, m - D) = 0$，损失为 0
   → 说明“已经够远了”，不再强求更远
- 如果 **D < m**：
   → 损失为 $(m - D)^2$
   → 也就是“距离越小、惩罚越大”，促使网络把它们再**往远拉**，直到 ≥ m

直觉：

> 不相似的样本至少要间隔一个 margin，不能挤得太近。

------

### 1.4 梳理一下梯度方向（更直观）

- 对正样本对（y=1）：
  - 损失是 $D^2$，会产生梯度把 $z_1, z_2$ 往一起拉 → 聚类
- 对负样本对（y=0）：
  - 当 $D < m$ 时，有梯度，拉远
  - 当 $D ≥ m$ 时，无梯度，不再“浪费力气”拉得更远

------

### 1.5 为啥要 margin？

如果没有 margin，负样本对的损失可能写成 $L_\text{neg} = 1/D^2$ 或类似东西，那会持续鼓励网络把负样本对拉得「无限远」，既没必要，也容易数值不稳定；margin 的作用就是：

> 够远了就别拉了，让网络把精力用在 **难区分的样本** 上。

------

## 二、“信息论版”对比损失：InfoNCE / NT-Xent

现代自监督对比学习（比如 SimCLR、MoCo）用的是 **另一种形式的对比损失**，思想仍然是“拉近正对、推远负对”，但用的是 **softmax + 交叉熵** 的方式。

### 2.1 场景设定

批量里有 $N$ 个样本，每个样本做两种增强 → 得到 $2N$ 个视图。

对其中一个样本 i 的两个视图，记为 $(z_i, z_i^+)$：

- 它们是 **正样本对**（同一个原图的不同增强）
- 批量里的其它 $2N-2$ 个视图当作 **负样本**

定义相似度（一般是余弦相似度）：
$$
\text{sim}(z_i, z_j) = \frac{z_i^\top z_j}{\|z_i\|\|z_j\|}
$$
再用一个温度参数 $\tau$ 进行缩放。

### 2.2 InfoNCE / NT-Xent 损失公式

对 i 这个样本，损失：
$$
L_i = - \log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\sum_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}
$$

- 分子：正样本对的相似度 → 希望越大越好
- 分母：对 i 来说，所有候选（正 + 负）相似度的 softmax 和
- 这其实就是：把“谁是 i 的正样本”当作一个分类问题，用交叉熵训练

------

### 2.3 和前面的“margin 对比损失”对比一下

- **经典对比损失（Siamese 版）**：
  - 显式用距离 + margin
  - 一次只看一对样本（或者一个 batch 内遍历对）
- **InfoNCE / NT-Xent**：
  - 用 softmax 把一个样本的正样本当作“正确类别”，负样本是“错误类别”
  - 一次考虑“一个 anchor vs 所有其它样本”的关系，更充分利用 batch 内所有负样本
  - 训练出来的表示在大规模自监督任务中效果很好

本质上两者目标是一致的：
 **拉近正对、拉远负对，只是数学实现形式不一样**。

------

## 三、对比损失常见应用场景

1. **度量学习（Metric Learning）**
   - 人脸验证（同一人/不同人）
   - 图像检索（相似图片搜索）
   - 通过对比损失让 embedding 空间有“距离含义”
2. **Siamese 网络**
   - 两个子网络共享参数，分别处理 $x_1, x_2$
   - 输出嵌入后，用对比损失训练“相似度度量”
3. **自监督对比学习**
   - 没有标签，只靠不同增强视图之间的对比
   - SimCLR、MoCo、BYOL、SimSiam 等
4. **对比式表示学习 + 下游任务**
   - 先用对比损失预训练一个 encoder
   - 再在下游分类/检测任务上“微调”

------

## 四、和 Triplet Loss 的区别（顺带提一句）

另外一个和对比损失经常一起出现的是 **triplet loss（三元组损失）**：

- anchor：$x_a$
- positive：$x_p$
- negative：$x_n$

目标是：
$$
\|f(x_a) - f(x_p)\|_2^2 + m \le \|f(x_a) - f(x_n)\|_2^2
$$
对应损失：
$$
L = \max\Big(0,\ \|z_a - z_p\|_2^2 - \|z_a - z_n\|_2^2 + m \Big)
$$
对比损失是 **“二元关系”**（一对样本 + 标签），
 Triplet 是 **“三元关系”**（锚点 + 正样本 + 负样本）。

------

## 五、总结

可以这样记：

1. **基本思想**：
    对比损失 = 让相似的样本向量更近，不相似的更远。

2. **经典公式（成对 + margin）：**
   $$
   L = y D^2 + (1-y)\max(0, m-D)^2
   $$

3. **现代 InfoNCE/NT-Xent：**
    用 softmax + 交叉熵，把“从一堆候选里选出那个正样本”的问题当分类来做



参考：

[对比损失]: https://www.zywvvd.com/notes/study/deep-learning/loss/contrastive-loss/contrastive-loss/

