[TOC]

转置卷积”（Transposed Convolution），也叫 **反卷积（Deconvolution）**，是深度学习中图像生成、上采样常用的一个操作，尤其广泛应用于：

- **生成对抗网络（GAN）**
- **语义分割（SegNet、UNet）**
- **自编码器（Decoder）**

它其实不是“真正的卷积逆操作”，但从“**恢复空间尺寸**”的角度来看，确实是“卷积的逆”。

------

#  一、什么是转置卷积（Transposed Convolution）

### ▶️ 简洁定义：

> 转置卷积是一种 **可以把小特征图“放大”**（恢复到更高分辨率）的操作，类似于卷积的逆过程。

# 二、转置卷积的尺寸公式

设：

- 输入大小：$I$
- 卷积核大小：$K$
- 步幅（stride）：$S$
- padding：$P$
- output_padding（用于调整输出尺寸）：$OP$

## 转置卷积的输出尺寸：

$$
O = (I - 1) \times S - 2P + K + OP
$$

举个例子：

```
# 输入是 2x2 特征图
# kernel_size = 3
# stride = 2
# padding = 1
# output_padding = 1

output_size = (2 - 1) * 2 - 2*1 + 3 + 1 = 4
```

最终输出是 4x4。

# 三、转置卷积的核心步骤

## 1. 总规则（很关键）：

对于输入中每个位置 `(i, j)` 上的值 `x = X[i, j]`：

1. 先算它在输出上对齐的“左上角位置”：
   - 行方向偏移：`row = i * stride`
   - 列方向偏移：`col = j * stride`
2. 然后在输出 `Y` 上，从 `(row, col)` 开始的 `kernel_size × kernel_size` 区域里，加上 `x * K`（逐元素相乘后相加）。
3. 如果多个输入位置对应的“印章”区域有重叠，就把值**累加**起来。

## 2. 示例：逐步演算

### 2.1 确定配置

我们用一个特别好算的配置：

- 输入特征图：大小 **2×2**
- 卷积核（kernel）：大小 **2×2**
- 步长（stride）：**2**
- padding = 0，output_padding = 0（先不搞复杂的）
- 通道都设为 1（只看 2D）

#### 1️⃣ 输入特征图 `X`

```
X = [[1, 2],
     [3, 4]]
```

#### 2️⃣ 卷积核 `K`（转置卷积层的权重）

取一个简单点的：

```
K = [[1, 0],
     [0, 1]]
```

#### 3️⃣ 先算好输出尺寸

对“转置卷积”，在这种配置下（stride = 2, padding=0, kernel=2, output_padding=0）：
$$
H_{out} = (H_{in} - 1) \times S + K = (2 - 1)\times 2 + 2 = 4
$$
所以输出是一个 **4×4** 的特征图。

我们先准备一个全 0 的 4×4 作为“画布”：

```
Y = [[0, 0, 0, 0],
     [0, 0, 0, 0],
     [0, 0, 0, 0],
     [0, 0, 0, 0]]
```

### 2.2 具体步骤

#### 🔹 输入元素 (0,0) = 1

- 位置：`i = 0, j = 0`
- 偏移：`row = 0*2 = 0`, `col = 0*2 = 0`
- 要加到输出 Y 的区域是：行 `[0,1]`，列 `[0,1]` 上的 2×2 小块。
- 要加的东西是：`1 * K`，也就是：

```
1 * K = [[1, 0],
         [0, 1]]
```

把它盖到 Y 上：

```
Y 原来：
[[0, 0, 0, 0],
 [0, 0, 0, 0],
 [0, 0, 0, 0],
 [0, 0, 0, 0]]

加到左上角 2×2 块后：

[[1, 0, 0, 0],
 [0, 1, 0, 0],
 [0, 0, 0, 0],
 [0, 0, 0, 0]]
```

------

#### 🔹 输入元素 (0,1) = 2

- 位置：`i = 0, j = 1`
- 偏移：`row = 0*2 = 0`, `col = 1*2 = 2`
- 要加的区域：Y 的行 `[0,1]`，列 `[2,3]`
- 要加的 patch：`2 * K`：

```
2 * K = [[2, 0],
         [0, 2]]
```

累加后：

```
Y 变为：

[[1, 0, 2, 0],
 [0, 1, 0, 2],
 [0, 0, 0, 0],
 [0, 0, 0, 0]]
```

------

#### 🔹 输入元素 (1,0) = 3

- 位置：`i = 1, j = 0`
- 偏移：`row = 1*2 = 2`, `col = 0*2 = 0`
- 区域：Y 的行 `[2,3]`，列 `[0,1]`
- patch：`3 * K`：

```
3 * K = [[3, 0],
         [0, 3]]
```

累加后：

```
Y 变为：

[[1, 0, 2, 0],
 [0, 1, 0, 2],
 [3, 0, 0, 0],
 [0, 3, 0, 0]]
```

------

#### 🔹 输入元素 (1,1) = 4

- 位置：`i = 1, j = 1`
- 偏移：`row = 1*2 = 2`, `col = 1*2 = 2`
- 区域：Y 的行 `[2,3]`，列 `[2,3]`
- patch：`4 * K`：

```
4 * K = [[4, 0],
         [0, 4]]
```

累加后最终的 Y：

```
Y = [[1, 0, 2, 0],
     [0, 1, 0, 2],
     [3, 0, 4, 0],
     [0, 3, 0, 4]]
```

这就是一次完整的 **2×2 → 4×4 的转置卷积过程**。

## 3. 和“插 0 再卷积”那种讲法的关系

很多书/博客会这么描述转置卷积：

1. 在输入元素之间插入 **stride − 1 个 0**，相当于把特征图“拉开”，变成更大的稀疏图；
2. 再对这个稀疏图做一个 **普通卷积**。

对我们这个例子来说：

- 原来 `2×2`，stride=2 → 插 0 后变成一个 `3×3` 稀疏图：

  ```
  [[1, 0, 2],
   [0, 0, 0],
   [3, 0, 4]]
  ```

- 再组合合适的 padding，用普通卷积扫过去，最终得到 `4×4`。

这两种理解本质是等价的，只是“**每个输入像素向外扩散一块**” vs “**插零 + 普通卷积**”两个视角。

## 4. 小结（你可以当成“记忆卡片”）

- **干什么用？**
   把小特征图变大（上采样），常用在解码器、分割、GAN。
- **怎么做？**
   对每个输入像素，拿卷积核乘一下，在输出上“盖一块”，然后把所有块加在一起。
- **步长 stride**：决定这些块之间的“间距”（稀疏还是密）。
- **padding/output_padding**：决定最后输出尺寸是否要再裁掉/补一点。



# 四、扩展

这种理解思维角度不一样，可以不这么理解

- 普通卷积：
   大图 → 小图，
   可以写成 `y = W x`（`W` 是一个很大的稀疏矩阵）。

- 转置卷积：
   小图 → 大图，
   就是用 `W^T` 去乘：
  $$
  x' = W^T y
  $$

也因此叫 **Transposed Convolution（矩阵转置后的卷积）**。

## 1. 一维的情况

### 1️⃣ 先看普通卷积

假设：

- 输入：$x = [x_1, x_2, x_3, x_4]$（长度 4）
- 卷积核（真正的 kernel）：$k = [w_1, w_2, w_3]$（长度 3）
- `stride = 1`，`padding = 0`，做 **valid 卷积**

卷积结果 $y$ 长度为 2，计算是：
$$
\begin{aligned}
y_1 &= w_1 x_1 + w_2 x_2 + w_3 x_3 \\
y_2 &= w_1 x_2 + w_2 x_3 + w_3 x_4
\end{aligned}
$$
这就是你熟悉的“滑动窗口卷积”。

------

### 2️⃣ 写成矩阵乘法：这里的 W 是谁？

我们想写成：
$$
y = W x
$$
那就把上面两行式子，写成矩阵形式：
$$
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
w_1 & w_2 & w_3 & 0 \\
0   & w_1 & w_2 & w_3
\end{bmatrix}
}_{W}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
$$
这里这个 **2×4 的大矩阵就是我说的 W**：

- 它的元素来自卷积核参数 $[w_1, w_2, w_3]$，
- 但卷积核在“滑动”的过程中，被 **复制 + 平移**，形成了这个“带状”的稀疏矩阵，
- 真正的卷积运算，其实就是拿这个 W 去乘 x。

所以：

- **卷积核 k：** 是那一小段 $[w_1, w_2, w_3]$
- **矩阵 W：** 是用 k 的值，按卷积滑动规则“铺”成的线性变换矩阵

------

### 3️⃣ 那转置卷积里的 $W^T$ 又是啥？

转置卷积里我写：
$$
x' = W^T y
$$
这里的 $W^T$ 就是上面这个矩阵的转置：
$$
W^T =
\begin{bmatrix}
w_1 & 0   \\
w_2 & w_1 \\
w_3 & w_2 \\
0   & w_3
\end{bmatrix}
$$

- 它拿着“小图” $y$ 乘一下，就能得到一个更长的向量 $x'$，
- 这个过程，对应的就是你在框架里看到的 **转置卷积（ConvTranspose）**。

------

### 4️⃣ 总结帮你捋一下

- **卷积核（kernel）**：模型真正学习的那一小块参数（例如 3×3 的小窗口）。
- **W**：用这个 kernel 按“滑动卷积”的方式，铺出来的一个大稀疏矩阵，用来表示整个卷积操作的线性变换。
- **转置卷积**：就是用这个大矩阵的转置 $W^T$ 去乘小图，从而得到大图，所以叫 **Transposed Convolution**。

👉 所以，你图里那句 “y = Wx / x' = W^T y” 里的 **W 不是单个卷积核本身，而是由卷积核参数构造出来的“大型卷积矩阵”**。实际框架里不会真的把 W 显式算出来，只是理论上可以这么看



## 2. 二维的情况

> **输入是 4×4 的矩阵时，W 是一个由卷积核“铺”出来的巨大稀疏矩阵。**
>  不同的是：
>
> - 先把 4×4 展平成长度 16 的向量 x
> - 输出 3×3 展平成长度 9 的向量 y
> - 卷积可以写成：  **y = W x，其中 W 的大小是 9×16**

下面一步步来看具体长什么样。

### 1️⃣ 设一个具体的 2D 卷积问题

我们设：

- 输入图片 $X$：4×4
- 卷积核（真正的 kernel）：2×2
- stride = 1，padding = 0
- 单通道 in/out（不考虑多个通道）

#### 输入 $X$

我们先给每个格子起个名字方便写：
$$
X = 
\begin{bmatrix}
x_{11} & x_{12} & x_{13} & x_{14} \\
x_{21} & x_{22} & x_{23} & x_{24} \\
x_{31} & x_{32} & x_{33} & x_{34} \\
x_{41} & x_{42} & x_{43} & x_{44}
\end{bmatrix}
$$

#### 卷积核 $K$

$$
K = 
\begin{bmatrix}
k_{11} & k_{12} \\
k_{21} & k_{22}
\end{bmatrix}
$$

------

### 2️⃣ 普通卷积输出的尺寸

- 输入 4×4
- kernel 2×2
- stride 1，padding 0

输出特征图尺寸：
$$
H_{out} = 4 - 2 + 1 = 3,\quad W_{out} = 4 - 2 + 1 = 3
$$
所以输出是一个 **3×3** 的矩阵：
$$
Y = 
\begin{bmatrix}
y_{11} & y_{12} & y_{13} \\
y_{21} & y_{22} & y_{23} \\
y_{31} & y_{32} & y_{33}
\end{bmatrix}
$$

------

### 3️⃣ 先写出卷积的 9 个位置的公式

比如左上角的 $y_{11}$：
 对应的是输入左上角那块 2×2：
$$
y_{11} = k_{11}x_{11} + k_{12}x_{12} + k_{21}x_{21} + k_{22}x_{22}
$$
右移一格，对应 $y_{12}$：
$$
y_{12} = k_{11}x_{12} + k_{12}x_{13} + k_{21}x_{22} + k_{22}x_{23}
$$
再右移一格，$y_{13}$：
$$
y_{13} = k_{11}x_{13} + k_{12}x_{14} + k_{21}x_{23} + k_{22}x_{24}
$$
第二行左边 $y_{21}$ 对应输入从 (2,1) 开始的 2×2：
$$
y_{21} = k_{11}x_{21} + k_{12}x_{22} + k_{21}x_{31} + k_{22}x_{32}
$$
……一直到 $y_{33}$，总共 9 个式子。

------

### 4️⃣ 把 2D 展平成 1D：向量 x 和向量 y

我们约定按“按行展开”（row-major）：

#### 把 $X$ 展平成长度 16 的列向量 $x$

$$
x =
\begin{bmatrix}
x_{11} \\ x_{12} \\ x_{13} \\ x_{14} \\
x_{21} \\ x_{22} \\ x_{23} \\ x_{24} \\
x_{31} \\ x_{32} \\ x_{33} \\ x_{34} \\
x_{41} \\ x_{42} \\ x_{43} \\ x_{44}
\end{bmatrix}
\in \mathbb{R}^{16}
$$

#### 把 $Y$ 展平成长度 9 的列向量 $y$

$$
y =
\begin{bmatrix}
y_{11} \\ y_{12} \\ y_{13} \\
y_{21} \\ y_{22} \\ y_{23} \\
y_{31} \\ y_{32} \\ y_{33}
\end{bmatrix}
\in \mathbb{R}^{9}
$$

------

### 5️⃣ 这时卷积就可以写成矩阵乘法：y = W x

W 的尺寸：**9×16**。
 我们只写前几行，你就能看出规律了。

#### 第一行：对应 $y_{11}$

我们刚才有：
$$
y_{11} = k_{11}x_{11} + k_{12}x_{12} + k_{21}x_{21} + k_{22}x_{22}
$$
那对应到向量 x 的下标位置是：

- $x_{11} \Rightarrow x$ 的第 1 个元素
- $x_{12} \Rightarrow x$ 的第 2 个元素
- $x_{21} \Rightarrow x$ 的第 5 个元素
- $x_{22} \Rightarrow x$ 的第 6 个元素

所以 W 的第一行应该是：
$$
W_{1,:} = [\,k_{11},\ k_{12},\ 0,\ 0,\ k_{21},\ k_{22},\ 0,\ 0,\ 0,\dots,0\,]
$$
也就是 16 维的行向量，只在这 4 个位置上是卷积核的值，其余全是 0。

------

#### 第二行：对应 $y_{12}$

$$
y_{12} = k_{11}x_{12} + k_{12}x_{13} + k_{21}x_{22} + k_{22}x_{23}
$$

对应到 x 的位置：

- $x_{12}$ → 第 2 个
- $x_{13}$ → 第 3 个
- $x_{22}$ → 第 6 个
- $x_{23}$ → 第 7 个

所以第二行是：
$$
W_{2,:} = [\,0,\ k_{11},\ k_{12},\ 0,\ 0,\ k_{21},\ k_{22},\ 0,\ 0,\dots,0\,]
$$
你可以看到：**和第一行比，就是整体向右“平移了一个格子”**。

------

#### 第三行：对应 $y_{13}$

$$
y_{13} = k_{11}x_{13} + k_{12}x_{14} + k_{21}x_{23} + k_{22}x_{24}
$$

对应 x 的下标是 3,4,7,8，于是：
$$
W_{3,:} = [\,0,\ 0,\ k_{11},\ k_{12},\ 0,\ 0,\ k_{21},\ k_{22},\ 0,\dots,0\,]
$$
你已经能看出来：
 **前 3 行对应第一行的 3 个卷积位置，卷积核在大矩阵里水平“滑动”；
 后 6 行则对应下面两行的 6 个位置，卷积核在大矩阵里垂直加水平“滑动”。**

完整的 W 会是一个这样的 9×16 稀疏矩阵，每一行都是 kernel [k11,k12;k21,k22] 摊成长度 4 的小片，放到对应窗格的位置上，其余全 0。

------

### 6️⃣ 转置卷积时用的就是 Wᵀ

- 普通卷积：
   展平后 $ y = W x$
   （大图 4×4 → 小图 3×3）
- 转置卷积（上采样）：
   展平后 $ x' = W^T y$
   （小图 3×3 → 大图 4×4）

也就是：
 **W 由卷积核 K“反复复制 + 平移”生成（大稀疏矩阵），
 转置卷积就是用这个大矩阵的转置来算。**

------

#### 小结一句话帮你记

> **2D 输入是 4×4 时，真正的 kernel 还是 2×2 那一小块；
>  我写公式里的 W，则是把这 2×2 kernel 按滑动窗口规则，堆成的一个 9×16 的稀疏大矩阵。**