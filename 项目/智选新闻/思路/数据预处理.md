## 1. 从源文件中加载数据集，拆分数据集为训练数据、测试数据和评估数据

```
# 1. 从文件中加载数据集
        dataset_dict = load_dataset("csv",data_files=str(RAW_DATA_DIR/RAW_DATA_FILE))
        # 训练数据拆分为训练集和测试集
        dataset_dict = dataset_dict["train"].train_test_split(test_size=DatasetConfig.test_split_ratio,seed=self.datasetConfig.seed)
        # 将训练集数据在拆分成训练集和验证集
        train_val = dataset_dict["train"].train_test_split(train_size=DatasetConfig.train_split_ratio,seed=self.datasetConfig.seed)
        # print(train_val)
        # print(dataset_dict)
        dataset_dict["train"],dataset_dict["valid"] = train_val["train"],train_val["test"]

```

## 2. 对分类问题从数据集中构建标签映射

```
# 构建label2id和id2label
def build_cat_map(self,train_ds) -> (Dict[str, int], Dict[int, str]):
    all_cat = set()
    for label in train_ds["category"]:
        all_cat.add(label)
    cat_list = sorted(all_cat)
    # 保存标签到文件(不存在才保存)
    if not os.path.exists(str(RAW_DATA_DIR/LABELS_FILE)):
        with open(RAW_DATA_DIR/LABELS_FILE,"w",encoding="utf-8") as f:
            f.write("\n".join(cat_list))
    cat2id = {label: id for id,label in enumerate(cat_list)}
    id2cat = {id : label for id,label in enumerate(cat_list)}
    return cat2id, id2cat
```

## 3. 将数据集id化并保存

```
# 将数据集转成模型输入数据
def _map_fn(batch):
    return self.tokenize_and_labels(batch, self.tokenizer,cat2id)
train_tok = dataset_dict["train"].map(_map_fn,batched=True,remove_columns=dataset_dict["train"].column_names)
test_tok = dataset_dict["test"].map(_map_fn,batched=True,remove_columns=dataset_dict["test"].column_names)
valid_tok = dataset_dict["valid"].map(_map_fn,batched=True,remove_columns=dataset_dict["valid"].column_names)

ds_dict = DatasetDict({
    "train": train_tok,
    "valid": test_tok,
    "test": valid_tok,
})
# 保存数据集（不存在保存）
ds_dict.save_to_disk(PROCESSED_DATA_DIR)
```

转换函数：生成{"inputIds":[],"summary":[],"category":[]}

```
def tokenize_and_labels(self,batch,tokenizer,cat2id):
    result = {}
    tokenized = tokenizer(
        batch["text"],
        truncation=True,
        max_length=self.datasetConfig.text_max_len,
    )
    result["input_ids"] = tokenized["input_ids"]
    # 将分类标签id化
    category_ids = [cat2id.get(category) for category in batch["category"]]
    result["category"] = category_ids
    #将摘要id化
    summary_tokenized = tokenizer(
        batch["summary"],
        truncation=True,
        max_length=self.datasetConfig.summary_max_len,
    )
    result["summary"] = summary_tokenized["input_ids"]
    return result
```

注意：填充数据可放到获取数据加载器进行，根据每一批次进行填充，而不是全局

## 4.获取数据加载器

将数据转换成模型输入的数据并填充，数据格式为{"input_ids"[],"attention_mask":[],"labels":[]}

```
# 获取数据加载器
def get_dataloader(self, ds_type="train",task="category"):
    dataset = None
    if (not any(PROCESSED_DATA_DIR.iterdir())):
        ds_dict = self.preprocess()
        if(ds_type == "train"):
            dataset = ds_dict["train"]
        elif(ds_type == "valid"):
            dataset = ds_dict["valid"]
        else:
            dataset = ds_dict["test"]
    else:
        # 加载数据集
        path = str(PROCESSED_DATA_DIR / ds_type)
        dataset = load_from_disk(path)

    # 设置格式为tensor
    dataset.set_format(type='torch')


    # 填充input_ids，生成attention_mask,用-100填充标签
    def collate_fn(batch):
        # 填充input_ids
        input_ids = [torch.tensor(data["input_ids"]) for data in batch]
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)
        # 生成attention_mask
        attention_mask = (input_ids != self.tokenizer.pad_token_id).int()
        # 处理标签
        # 分类
        labels = None
        if(task == "category"):
            labels = [torch.tensor(data["category"]) for data in batch]
        elif(task == "summary"):
            # 摘要，需要用-100填充
            labels = [torch.tensor(data["summary"]) for data in batch]
            # 填充
            labels = pad_sequence(labels, batch_first=True, padding_value=-100)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

    # 创建DataLoader
    dataloader = DataLoader(dataset=dataset,batch_size=self.datasetConfig.batch_size,shuffle=True,collate_fn=collate_fn)
    return dataloader
```