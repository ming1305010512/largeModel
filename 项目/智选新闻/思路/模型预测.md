```
# -*- coding: utf-8 -*-
import torch
from transformers import AutoTokenizer, BartForConditionalGeneration

from configuration.config import *
from process.datasetor import Datasetor, DatasetConfig
from runner.model_def import BartEncoderClassifier, ModelConfig
from runner.train_cls import TrainClsConfig
from runner.train_summary import TrainSummaryConfig


class Predictor:
    def __init__(self,model,tokenizer,device,datasetor):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        self.datasetor = datasetor

    @torch.no_grad()
    def predict_category(self, text:str):
        cat2id,id2cat = self.datasetor.get_cat_label_map()
        inputs = self.tokenizer(text, return_tensors="pt",truncation=True, max_length=self.datasetor.datasetConfig.text_max_len)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        inputs =  {k:v for k,v in inputs.items() if k in ["input_ids", "attention_mask"]}
        outputs = self.model(**inputs)
        # [1,c]
        logits = outputs.get("logits")
        pred = logits.argmax(dim=-1).item()
        return id2cat[pred]

    def predict_summary(self,text:str):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True,max_length=self.datasetor.datasetConfig.text_max_len)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        inputs = {k: v for k, v in inputs.items() if k in ["input_ids", "attention_mask"]}
        out_ids = (self.model.generate(**inputs,
                       num_beams=3,
                       max_new_tokens=128,
                       # 最多生成多少个“新 token”（只算 decoder 生成的长度，不含输入）
                       early_stopping=True,
                       # 配合 beam search 使用，控制何时停止搜索,当系统认为“继续扩展也不可能比当前最好的完整句子更好”时，就提前停
                       no_repeat_ngram_size=3,
                       # 禁止重复 n-gram，防止“复读机”。如果设为 3，
                       # 含义是：生成过程中不允许出现任何重复的连续3 个 token 的片段,
                       # 一旦候选下一个 token 会导致某个 3-gram 重复，就把这个 token 的概率置为 -inf（不可选）
                       length_penalty=1,  # 长度惩罚/长度偏好（主要影响 beam search 的“序列得分”）。
                   ))

        return self.tokenizer.decode(out_ids[0], skip_special_tokens=True).replace(" ","")

def predict_cls():
    # 1. 定义设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # 2. 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
    # 3. 加载微调后的模型
    datasetconfig = DatasetConfig()
    datasetor = Datasetor(datasetconfig)
    label2id, id2label = datasetor.get_cat_label_map()
    modelConfig = ModelConfig()
    model = BartEncoderClassifier(modelConfig, id2label).to(device)
    model.load_state_dict(torch.load(CLS_BAST_MODEL / "cls_model_best.pt",map_location=device))
    predictor = Predictor(model,datasetor.tokenizer,device,datasetor)
    text = "本报讯(记者邓姣)老公去世了近十年，她就独自赡养着婆婆近十年。其间，她的两个儿子锒铛入狱，她又身患癌症。她仍对婆婆不离不弃，两人相濡以沫，靠着一年1800元的卖菜收入，维持生活。而邻居评价她们，比亲娘俩还亲。今年63岁的叶应芬，在九龙坡区含谷镇新营防村很出名，大家都评价她：能干、孝顺！每天天不亮，凌晨5点左右，叶应芬就早早起床，在路途中“拦截”上街的菜农，从他们手中买下一箩筐的菜，自己再挑到街上卖。每斤可赚几分钱的差价。看着叶应芬起早贪黑，80多岁的老母亲也闲不住，帮忙煮饭和照看家"
    cls = predictor.predict_category(text)
    print("分类",cls)

def predict_summary():
    # 1. 定义设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # 2. 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
    # 3. 加载微调后的模型
    datasetconfig = DatasetConfig()
    datasetor = Datasetor(datasetconfig)
    trainSummaryConfig = TrainSummaryConfig()
    # 3. 加载微调后的模型
    model = BartForConditionalGeneration.from_pretrained(
        SUMMARY_BAST_MODEL,
    ).to(device)
    predictor = Predictor(model,datasetor.tokenizer,device,datasetor)
    text = "本报讯(记者邓姣)老公去世了近十年，她就独自赡养着婆婆近十年。其间，她的两个儿子锒铛入狱，她又身患癌症。她仍对婆婆不离不弃，两人相濡以沫，靠着一年1800元的卖菜收入，维持生活。而邻居评价她们，比亲娘俩还亲。今年63岁的叶应芬，在九龙坡区含谷镇新营防村很出名，大家都评价她：能干、孝顺！每天天不亮，凌晨5点左右，叶应芬就早早起床，在路途中“拦截”上街的菜农，从他们手中买下一箩筐的菜，自己再挑到街上卖。每斤可赚几分钱的差价。看着叶应芬起早贪黑，80多岁的老母亲也闲不住，帮忙煮饭和照看家"
    summary = predictor.predict_summary(text)
    print("摘要",summary)

if __name__ == "__main__":
    predict_cls()
    predict_summary()
```