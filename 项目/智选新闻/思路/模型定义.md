## 1. 基于BART encoder的分类模型

使用模型：fnlp/bart-base-chinese

1. 通过get_encoder()获取encoder编码器，增加一层线性头，用于分类

2. BART encoder 没有像 BERT 那样固定的 `[CLS]` 语义 token，所以通常用：

   - **取第一个 token 的 hidden**（类似 CLS 的做法）

   - 或者 **mean pooling**（更稳一点）

     这里使用mean pooling

```
class BartEncoderClassifier(nn.Module):
    def __init__(self, config,cat_list):
        super(BartEncoderClassifier, self).__init__()
        self.encoder = BartModel.from_pretrained(config.model_path).get_encoder()
        self.dropout = nn.Dropout(config.dropout)
        # 最后一层线性头,用于分类
        self.classifier = nn.Linear(self.encoder.config.hidden_size, len(cat_list))

    # BART encoder 没有像 BERT 那样固定的 `[CLS]` 语义 token,可以取第一个token
    # 的hidden，不过做平均池化更好一些
    def mean_pooling(self, last_hidden, attention_mask):
        # last_hidden形状[b,seq,d],mask形状[b,seq,d]
        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        # 形状[b,d]
        summed = (last_hidden * mask).sum(dim=1)
        # 将0替换成一个比较小的值，防止都是0除数变为0，求列维度中为1的总数（实际seq长度）
        denom = mask.sum(dim=1).clamp(min=1e-9)
        return summed / denom

    # 前向传播
    def forward(self, input_ids=None, attention_mask=None, labels=None):
        # 执行bart的encoder
        enc_out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        # [b, s, d]
        last_hidden_state = enc_out.last_hidden_state
        # 做平均池化[b,d]
        pooled = self.mean_pooling(last_hidden_state, attention_mask)
        # 执行线性层，获取最终输[b,c]
        logits = self.classifier(self.dropout(pooled))

        # 计算损失值,labels形状[b,1]
        loss = None
        if labels is not None:
            loss = CrossEntropyLoss()(logits, labels)
        return {"loss": loss,"logits": logits}
```