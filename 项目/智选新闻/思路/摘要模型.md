

模型训练时直接使用BERT_MODEL_NAME = "fnlp/bart-base-chinese"

```
model = BartForConditionalGeneration.from_pretrained(
    BERT_MODEL_NAME,
).to(device)
```

也可以自定义模型：

1. 前向传播使用bart的编码器和解码器，分开进行，最后加一层线性层用于生成token
2. 自定义了generate用于生成摘要，包含了两种实现方法，一种束搜索，一种贪婪搜索

```
class CustomSummarizeModel(nn.Module):
        """摘要模型"""

        compute_parameters = compute_parameters
        load_params = load_params

        def __init__(self, model_name: str):
                super().__init__()
                self.config = AutoConfig.from_pretrained(model_name)
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = BartModel.from_pretrained(model_name)
                # 语言模型头
                self.lm_head = nn.Linear(self.config.d_model, self.config.vocab_size)
                # 将语言模型头权重设置为共享权重，共享嵌入层权重
                self.lm_head.weight = self.model.shared.weight
                self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100)

        def forward(self, input_ids, attention_mask=None, labels=None):
                decoder_input_ids = None
                decoder_attention_mask = None
                # 如果提供了标签，将标签右移一位拼接在起始符之后，作为解码器输入
                if labels is not None:
                        decoder_input_ids = labels.new_zeros(labels.shape)
                        decoder_input_ids[:, 1:] = labels[:, :-1].clone()
                        decoder_input_ids[:, 0] = self.config.decoder_start_token_id
                        # 修改输入中的 -100 为 pad_token_id，防止 embedding 时报错
                        decoder_input_ids.masked_fill_(
                                decoder_input_ids == -100, self.config.pad_token_id
                        )
                        decoder_attention_mask = (
                                decoder_input_ids != self.config.pad_token_id
                        ).long()

                # 编码器前向传播
                encoder_outputs = self.model.encoder(input_ids, attention_mask)
                # 解码器前向传播
                outputs = self.model.decoder(
                        input_ids=decoder_input_ids,
                        attention_mask=decoder_attention_mask,
                        encoder_hidden_states=encoder_outputs.last_hidden_state,
                        encoder_attention_mask=attention_mask,
                        use_cache=False,
                )

                logits = self.lm_head(outputs.last_hidden_state)
                loss = None
                if labels is not None:
                        loss = self.loss_fn(
                                logits.view(-1, self.config.vocab_size), labels.view(-1)
                        )
                return {"loss": loss, "logits": logits}

        @torch.inference_mode()
        def generate(
                self,
                input_ids,
                attention_mask=None,
                max_length=128,
                num_beams=2,
        ):
                self.eval()
                # 贪婪搜索
                # return self.greedy_search(input_ids, attention_mask, max_length)
                # 束搜索
                return self.beam_search(input_ids, attention_mask, max_length, num_beams)

        def beam_search(self, input_ids, attention_mask, max_length, num_beams):
                """束搜索"""
                device = input_ids.device
                batch_size = input_ids.size(0)
                vocab_size = self.config.vocab_size

                # 编码器前向传播
                encoder_outputs = self.model.encoder(input_ids, attention_mask)
                # 复制编码器输出、编码器注意力掩码以匹配束数量
                encoder_hidden_states = encoder_outputs.last_hidden_state.repeat_interleave(
                        num_beams, dim=0
                )
                encoder_attention_mask = attention_mask.repeat_interleave(num_beams, dim=0)

                # 初始化解码输入为起始符
                # decoder_input_ids:[batch_size * beam, 1]
                decoder_input_ids = torch.full(
                        (batch_size * num_beams, 1),
                        self.config.decoder_start_token_id,
                        dtype=torch.long,
                        device=device,
                )

                # 定义束索引偏移量
                beam_offset = torch.arange(batch_size, device=device) * num_beams
                # 初始化束分数，起初只有每个样本的第一个束有效
                beam_scores = torch.full((batch_size * num_beams,), -1e9, device=device)
                beam_scores[beam_offset] = 0
                # 初始化完成状态
                done = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=device)

                for step in range(max_length):
                        # 解码当前输入
                        decoder_outputs = self.model.decoder(
                                input_ids=decoder_input_ids,
                                encoder_hidden_states=encoder_hidden_states,
                                encoder_attention_mask=encoder_attention_mask,
                                use_cache=False,
                        )
                        # 取最后一个时间步的 logits
                        logits = self.lm_head(decoder_outputs.last_hidden_state[:, -1, :])
                        # 计算每个候选的得分[B*K,V]
                        log_probs = nn.functional.log_softmax(logits, dim=-1)

                        # 禁止已经结束的束继续生成,done为(B*K)
                        log_probs[done] = -float("inf")
                        # 令已完成的束的 eos 对应的 log_prob 为 0，使其只能生成eos，同时也保证已完成的束得分不变
                        log_probs[done, self.config.eos_token_id] = 0

                        # 累积每个束的得分
                        log_probs += beam_scores.view(-1).unsqueeze(1)
                        # 重塑得分矩阵
                        log_probs = log_probs.view(batch_size, num_beams * vocab_size)
                        # 每个样本选择 top num_beams 个候选
                        beam_scores, indices = torch.topk(log_probs, num_beams, dim=1)
                        indices = indices.view(-1)

                        # 获取候选对应的束索引和 token id
                        beam_indices = indices // vocab_size
                        beam_indices += beam_offset.repeat_interleave(num_beams, dim=0)
                        token_ids = indices % vocab_size

                        # 更新输入序列
                        decoder_input_ids = torch.cat(
                                [decoder_input_ids[beam_indices], token_ids.view(-1, 1)], dim=1
                        )

                        # 更新完成状态，若所有序列完成则提前终止
                        done = token_ids.eq(self.config.eos_token_id) | done[beam_indices]
                        if done.all():
                                break

                # 选择每个样本beam_score最大的束
                best_indices = beam_scores.argmax(dim=-1) + beam_offset
                return decoder_input_ids[best_indices]

        def greedy_search(self, input_ids, attention_mask, max_length):
                """贪婪搜索"""
                batch_size = input_ids.size(0)
                device = input_ids.device

                # 编码器前向传播
                encoder_outputs = self.model.encoder(input_ids, attention_mask)

                # 初始化解码器输入
                decoder_input_ids = torch.full(
                        (batch_size, 1),
                        self.config.decoder_start_token_id,
                        dtype=torch.long,
                        device=device,
                )

                # 初始化完成状态
                done = torch.zeros(batch_size, dtype=torch.bool, device=device)

                # 逐步解码
                for step in range(max_length):
                        # 构建 attention mask
                        decoder_attention_mask = (
                                decoder_input_ids != self.config.pad_token_id
                        ).long()

                        # 解码器前向传播
                        decoder_outputs = self.model.decoder(
                                input_ids=decoder_input_ids,
                                attention_mask=decoder_attention_mask,
                                encoder_hidden_states=encoder_outputs.last_hidden_state,
                                encoder_attention_mask=attention_mask,
                                use_cache=False,
                        )
                        logits = self.lm_head(decoder_outputs.last_hidden_state)

                        # 取最后一步输出
                        next_token_logits = logits[:, -1, :]
                        next_tokens = torch.argmax(next_token_logits, dim=-1)

                        # 更新输出序列
                        decoder_input_ids = torch.cat(
                                [decoder_input_ids, next_tokens.unsqueeze(1)], dim=1
                        )

                        # 检查是否全部生成结束
                        done |= next_tokens == self.config.eos_token_id
                        if done.all():
                                break

                return decoder_input_ids

        def predict(self, text, device=torch.device("cpu"), batch_size=8):
                self.eval()
                self.to(device)

                res: list[str] = []
                # 统一转换为列表
                input_texts = text if isinstance(text, list) else [text]
                # 逐批次处理
                for i in range(0, len(input_texts), batch_size):
                        batch_texts = input_texts[i : i + batch_size]
                        inputs = self.tokenizer(
                                batch_texts,
                                max_length=1024,
                                truncation=True,
                                padding=True,
                                return_tensors="pt",
                        ).to(device)
                        outputs = self.generate(
                                input_ids=inputs["input_ids"],
                                attention_mask=inputs["attention_mask"],
                        )
                        batch_res = self.tokenizer.batch_decode(
                                outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True
                        )
                        batch_res = [a_res.replace(" ", "") for a_res in batch_res]
                        res.extend(batch_res)
                return res if isinstance(text, list) else res[0]
```