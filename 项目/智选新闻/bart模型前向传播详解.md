[TOC]

ä»¥ HuggingFace é‡Œæœ€å¸¸ç”¨çš„ **`BartForConditionalGeneration.forward()`** ä¸ºä¸»çº¿ï¼ŒæŠŠä¸€æ¬¡å‰å‘ **ä»è¾“å…¥åˆ° loss / logits** çš„å®Œæ•´é“¾è·¯è®²æ¸…æ¥šï¼ˆå«ï¼šshiftã€encoder/decoderã€cross-attnã€lm_headã€cacheï¼‰ã€‚

ä¸‹é¢çš„â€œæµç¨‹â€å¯ä»¥å½“æˆè¯»æºç çš„å¯¼å›¾ã€‚

------

## 0. ä½ åœ¨è°ƒç”¨ forward æ—¶ä¸€èˆ¬ä¼ ä»€ä¹ˆ

å¸¸è§ä¸¤ç§æ¨¡å¼ï¼š

### A) è®­ç»ƒï¼ˆæœ‰ labelsï¼‰

```python
out = model(
  input_ids=src_ids,
  attention_mask=src_mask,
  labels=tgt_ids
)
```

ä½ ä¸æ˜¾å¼ä¼  `decoder_input_ids`ï¼Œåªä¼  `labels`ã€‚

### B) æ¨ç†/ç”Ÿæˆï¼ˆæ—  labelsï¼‰

```python
out = model(
  input_ids=src_ids,
  attention_mask=src_mask,
  decoder_input_ids=dec_ids
)
```

------

## 1. forward çš„â€œå¤§æ¡†æ¶â€ï¼ˆä¸€å¥è¯ï¼‰

**BART forward = Encoder(src) â†’ Decoder(tgt, cross-attn to encoder) â†’ lm_head â†’ (å¯é€‰) loss**

------

## 2. å…³é”®æ­¥éª¤ 1ï¼šå¤„ç† decoder è¾“å…¥ï¼ˆshift rightï¼‰

### 2.1 å¦‚æœä½ ä¼ äº† `labels`

HF ä¼šåšä¸€ä»¶äº‹ï¼š**æŠŠ labels å³ç§»ä¸€ä½**ä½œä¸º decoder çš„è¾“å…¥ï¼ˆteacher forcingï¼‰ã€‚

- `labels`: ç›®æ ‡åºåˆ—ï¼ˆå¸Œæœ›æ¨¡å‹é¢„æµ‹å‡ºæ¥çš„ tokenï¼‰
- `decoder_input_ids`: å³ç§»åçš„ç›®æ ‡åºåˆ—ï¼ˆå–‚ç»™ decoder çš„â€œå·²çŸ¥å‰ç¼€â€ï¼‰

å³ç§»è§„åˆ™ï¼ˆæ¦‚å¿µï¼‰ï¼š

```
decoder_input_ids = [BOS] + labels[:-1]
```

åŒæ—¶ï¼Œ`labels` é‡Œå¦‚æœæœ‰ `-100`ï¼ˆå¿½ç•¥ä½ï¼‰ï¼Œloss è®¡ç®—ä¼šè·³è¿‡ã€‚

> ä½ è¦è®°ä½ï¼š
> **è®­ç»ƒæ—¶ decoder çœ‹åˆ°çš„æ˜¯æ­£ç¡®ç­”æ¡ˆçš„å‰ç¼€ï¼ˆteacher forcingï¼‰ï¼Œä¸æ˜¯è‡ªå·±é¢„æµ‹çš„å‰ç¼€ã€‚**

### 2.2 å¦‚æœä½ æ²¡ä¼  labelsï¼Œä½†ä¼ äº† decoder_input_ids

é‚£å°±ç›´æ¥ç”¨ä½ ç»™çš„ `decoder_input_ids`ï¼Œä¸ shiftã€‚

------

## 3. å…³é”®æ­¥éª¤ 2ï¼šEncoder å‰å‘

Encoder è¾“å…¥ï¼š

- `input_ids`ï¼šå½¢çŠ¶ `[bs, src_len]`
- `attention_mask`ï¼šå½¢çŠ¶ `[bs, src_len]`ï¼ˆ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤º paddingï¼‰

Encoder åšä»€ä¹ˆï¼š

- token embedding + position embedding
- N å±‚ Transformer Encoder blockï¼ˆself-attn + FFNï¼‰
- è¾“å‡º `encoder_last_hidden_state`ï¼šå½¢çŠ¶ `[bs, src_len, d_model]`

HF çš„è¿”å›é€šå¸¸æ˜¯ä¸€ä¸ªç»“æ„ä½“/tupleï¼š

- `encoder_outputs.last_hidden_state` å°±æ˜¯ä¸Šé¢çš„å¼ é‡

------

## 4. å…³é”®æ­¥éª¤ 3ï¼šDecoder å‰å‘ï¼ˆå« cross-attentionï¼‰

Decoder è¾“å…¥ï¼š

- `decoder_input_ids`ï¼š`[bs, tgt_len]`
- `decoder_attention_mask`ï¼š`[bs, tgt_len]`ï¼ˆæœ‰æ—¶ä½ ä¸ä¼ ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨æ ¹æ® pad æ¨ï¼‰
- `encoder_hidden_states`ï¼šæ¥è‡ª encoder çš„ `[bs, src_len, d_model]`
- `encoder_attention_mask`ï¼šä¹Ÿå°±æ˜¯ `attention_mask`

Decoder æ¯ä¸€å±‚åŒ…å«ä¸‰å—æ³¨æ„åŠ›ï¼š

### 4.1 Masked Self-Attentionï¼ˆè‡ªå›å½’ï¼‰

- åªèƒ½çœ‹è§è‡ªå·±å½“å‰ä½ç½®ä¹‹å‰çš„ tokenï¼ˆå› æœ maskï¼‰
- ä¿è¯ç”Ÿæˆæ—¶â€œä¸èƒ½å·çœ‹æœªæ¥â€

### 4.2 Cross-Attentionï¼ˆæœ€å…³é”®ï¼‰

- Query æ¥è‡ª decoder å½“å‰éšçŠ¶æ€
- Key/Value æ¥è‡ª encoder è¾“å‡º
- è®© decoder åœ¨ç”Ÿæˆæ—¶â€œå¯¹é½/æ£€ç´¢â€æºæ–‡æœ¬ä¿¡æ¯ï¼ˆç¿»è¯‘ã€æ‘˜è¦éƒ½é å®ƒï¼‰

### 4.3 FFN

- ä¸¤å±‚å‰é¦ˆç½‘ç»œ + æ¿€æ´» + dropout

Decoder è¾“å‡ºï¼š

- `decoder_last_hidden_state`: `[bs, tgt_len, d_model]`
- ï¼ˆå¯é€‰ï¼‰`past_key_values`: ç”¨äºå¢é‡ç”Ÿæˆ cache
- ï¼ˆå¯é€‰ï¼‰attentions / hidden_states

------

## 5. å…³é”®æ­¥éª¤ 4ï¼šlm_head å¾—åˆ° logits

BART ForConditionalGeneration ä¼šæŠŠ decoder è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨ï¼š

```
logits = lm_head(decoder_last_hidden_state)
```

`lm_head` æœ¬è´¨æ˜¯ä¸€ä¸ª**çº¿æ€§å±‚**ï¼š

```
lm_head: Linear(d_model â†’ vocab_size)
```

æ•°å­¦å½¢å¼ï¼š

```
logits = decoder_hidden_state @ Wáµ€ + b
```

- `W`: `[vocab_size, d_model]`
- `b`: `[vocab_size]`

ğŸ‘‰ **æŠŠâ€œè¯­ä¹‰ç©ºé—´â€æŠ•å½±å›â€œè¯è¡¨ç©ºé—´â€**

å½¢çŠ¶ï¼š

- `logits`: `[bs, tgt_len, vocab_size]`

æ³¨æ„ï¼šBART é€šå¸¸è¿˜ä¼šåŠ ä¸€ä¸ª `final_logits_bias`ï¼ˆä¸€ä¸ªè¯è¡¨å¤§å°çš„ biasï¼‰ï¼š

```
logits = logits + final_logits_bias
```

------

## 6. å…³é”®æ­¥éª¤ 5ï¼šlossï¼ˆåªåœ¨ä½ ä¼  labels æ—¶ï¼‰

loss è®¡ç®—æ–¹å¼ï¼š

- æŠŠ `logits` å±•å¹³æˆ `[bs*tgt_len, vocab]`
- æŠŠ `labels` å±•å¹³æˆ `[bs*tgt_len]`
- ç”¨ CrossEntropyLoss(ignore_index=-100)

ä¹Ÿå°±æ˜¯ï¼š

```
loss = CE(logits.view(-1, V), labels.view(-1))
```

è¾“å‡ºé‡Œä¼šåŒ…å«ï¼š

- `loss`ï¼ˆæ ‡é‡ï¼‰
- `logits`
- ä»¥åŠ encoder/decoder çš„å„ç§è¾“å‡º

------

## 7. cache / use_cache / past_key_valuesï¼ˆç”Ÿæˆæ—¶ä¸ºä»€ä¹ˆå¿«ï¼‰

åœ¨ç”Ÿæˆæ—¶ï¼ˆç‰¹åˆ«æ˜¯ `generate()` å†…éƒ¨ï¼‰ï¼Œä¼šè®¾ç½® `use_cache=True`ï¼š

- decoder æ¯ä¸€æ­¥ç”Ÿæˆåªæ–°å¢ 1 ä¸ª token
- å¦‚æœä¸ cacheï¼šæ¯ä¸€æ­¥éƒ½è¦å¯¹æ•´ä¸ªå†å²åºåˆ—åš attentionï¼Œä»£ä»·è¶Šæ¥è¶Šå¤§
- æœ‰ cacheï¼šä¿å­˜æ¯ä¸€å±‚çš„ K/Vï¼ˆpast_key_valuesï¼‰ï¼Œä¸‹ä¸€æ­¥åªç®—æ–° token çš„ Q å’Œæ–° K/V

æ‰€ä»¥ forward ä¼šæ”¯æŒï¼š

- è¾“å…¥ `past_key_values`
- è¾“å‡ºæ›´æ–°åçš„ `past_key_values`

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç”Ÿæˆæ—¶ `tgt_len` æ¯æ¬¡å¢ 1ï¼Œä½†é€Ÿåº¦è¿˜èƒ½æ¥å—ã€‚

------

## 8. ä¸€ä»½â€œæ¥è¿‘æºç â€çš„ä¼ªä»£ç ï¼ˆä½ è¯»æºç ä¼šéå¸¸å¯¹å¾—ä¸Šï¼‰

```python
def forward(
  input_ids, attention_mask,
  decoder_input_ids=None, decoder_attention_mask=None,
  labels=None,
  encoder_outputs=None,
  past_key_values=None,
  use_cache=None,
  output_attentions=None,
  output_hidden_states=None,
  return_dict=True
):

  # 1) å‡†å¤‡ decoder_input_ids
  if labels is not None and decoder_input_ids is None:
      decoder_input_ids = shift_right(labels)  # [BOS] + labels[:-1]

  # 2) encoderï¼ˆå¦‚æœæ²¡ç»™ encoder_outputsï¼‰
  if encoder_outputs is None:
      encoder_outputs = encoder(
        input_ids=input_ids,
        attention_mask=attention_mask,
        ...
      )
  enc = encoder_outputs.last_hidden_state  # [bs, src_len, d_model]

  # 3) decoder
  decoder_outputs = decoder(
    input_ids=decoder_input_ids,
    attention_mask=decoder_attention_mask,
    encoder_hidden_states=enc,
    encoder_attention_mask=attention_mask,
    past_key_values=past_key_values,
    use_cache=use_cache,
    ...
  )
  dec = decoder_outputs.last_hidden_state  # [bs, tgt_len, d_model]

  # 4) lm_head -> logits
  logits = lm_head(dec) + final_logits_bias  # [bs, tgt_len, vocab]

  # 5) lossï¼ˆå¯é€‰ï¼‰
  loss = None
  if labels is not None:
      loss = CE(logits.view(-1, V), labels.view(-1), ignore_index=-100)

  # 6) è¿”å›
  return Seq2SeqLMOutput(
    loss=loss,
    logits=logits,
    past_key_values=decoder_outputs.past_key_values,
    decoder_hidden_states=...,
    decoder_attentions=...,
    cross_attentions=...,
    encoder_last_hidden_state=enc,
    encoder_hidden_states=...,
    encoder_attentions=...
  )
```

------

## 9. ä½ æœ€å®¹æ˜“å¡çš„ 3 ä¸ªç‚¹ï¼ˆæˆ‘æå‰å¸®ä½ ç‚¹å‡ºæ¥ï¼‰

1. **labels vs decoder_input_ids**

- `labels` æ˜¯â€œè¦é¢„æµ‹çš„ç›®æ ‡â€
- `decoder_input_ids` æ˜¯â€œå–‚ç»™ decoder çš„è¾“å…¥â€
- è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ shiftï¼šdecoder è¾“å…¥ = labels å³ç§»

1. **cross-attention çš„æ¥æº**

- decoder çš„ cross-attn çš„ K/V æ¥è‡ª encoder è¾“å‡º
- attention_mask ä¼šå½±å“ encoder å“ªäº› token å¯è¢«å…³æ³¨ï¼ˆpadding ä¸è¯¥è¢«å…³æ³¨ï¼‰

1. **ç”Ÿæˆä¸ºä»€ä¹ˆè¦ past_key_values**

- å¦åˆ™æ¯ä¸€æ­¥éƒ½é‡ç®—å†å²ï¼Œå¤æ‚åº¦çˆ†ç‚¸
- cache è®©ç”Ÿæˆä» â€œO(TÂ²)â€ æ›´æ¥è¿‘ â€œé€æ­¥å¢é‡â€

