[TOC]

束搜索（Beam Search）是生成模型里最常用的解码算法之一，用来在 **“贪心太短视、全搜索太贵”** 之间折中。

------

## 1) 它到底在解决什么问题

生成模型每一步都会给出下一 token 的分布 $$(P(y_t \mid y_{<t}, x))。$$

- **贪心（Greedy）**：每一步都选当前概率最大的 token
  ✅ 快；❌ 容易走进局部最优（前面一步选错，后面回不来）
- **全搜索**：枚举所有序列
  ❌ 指数爆炸，不可能

**束搜索**：每一步保留“最有希望的 K 条候选”，继续扩展。
K 就是 **beam size（束宽）**。

------

## 2) 束搜索的核心思想（一句话）

> **在每个时间步，只保留累计得分最高的 K 条部分序列（hypotheses）**

它不只看“当前一步”的概率，而看“到目前为止整条序列”的累计得分。

------

## 3) 得分怎么算：log 概率累加（为什么用 log）

一条生成序列 (y_{1:T}) 的概率是连乘：
$$
P(y_{1:T}\mid x) = \prod_{t=1}^T P(y_t \mid y_{<t}, x)
$$
连乘很容易下溢、也不方便比较，所以用 log：
$$
\log P(y_{1:T}\mid x) = \sum_{t=1}^T \log P(y_t \mid y_{<t}, x)
$$
因此束搜索的“路径分数”一般是：
$$
score = \sum_{t=1}^T \log p_t
$$
（有时会加长度惩罚，后面说）

------

## 4) 一步一步讲束搜索流程（最标准版本）

设 beam size = K。

### 初始化（t=1）

- 当前只有一条空序列 `["<bos>"]`，score=0

### 每个时间步 t：

1. 对 beam 里每条候选序列，模型输出下一步的 logits → softmax 得到概率
2. 对每条候选，取 top-k_next 个可能的下一 token（通常取前 K 或更多）
3. 把所有候选扩展得到一堆新序列（数量约 K×k_next）
4. 对每条新序列，更新累计 score（加上新 token 的 log 概率）
5. 从所有新序列里选出 score 最高的 K 条，作为下一步的 beam
6. 如果出现 `<eos>`：
   - 一般把它放到“已完成序列集合”里（finished set）
   - beam 里仍保留未结束的继续扩展
7. 终止条件：
   - 达到 max_length
   - 或者 finished set 里已经有足够多高分序列，并且未完成的再怎么扩展也不可能超过它们（HF 有实现的早停策略）

最后从 finished set 里取分最高的作为输出（或返回 n-best）。

------

## 5) 用一个小例子让你“看到它怎么选”

假设 K=2（保留两条）

t=1：从 `<bos>` 出发

- A: 0.6
- B: 0.4
  保留：`<bos>A`、`<bos>B`

t=2：分别扩展
从 A：

- A→C: 0.6
- A→D: 0.4
  从 B：
- B→C: 0.9
- B→D: 0.1

累计概率（乘）：

- A C: 0.36
- A D: 0.24
- B C: 0.36
- B D: 0.04

此时 top2 是：`AC` 和 `BC`（并列）
你看到它**不会只沿着 A 一条走到底**，而是会保留 B 这条“次优分支”，因为它后续可能反超。

------

## 6) 为什么束搜索会偏好短句？长度惩罚（Length Penalty）

因为 log 概率是负数（概率 <1），越长累加越负，所以**短句容易得分更高**。

解决：加长度惩罚（最常见是 Google NMT 那种）：
$$
score'=\frac{\log P(y)}{(len(y))^\alpha}
$$
或类似形式。

- $$(\alpha=0)$$：无惩罚（更偏短）
- $$(\alpha>0)$$：鼓励更长一点（但太大会啰嗦）

在 HF `generate()` 里就是 `length_penalty`。

------

## 7) 束搜索 vs 采样（top-k/top-p）什么时候用哪个

### 束搜索（beam）

- ✅ 更确定、更“像答案”
- ✅ 摘要、翻译、信息抽取式生成更常用
- ❌ 容易“模板化”、多样性差
- ❌ 可能出现“重复”问题（尤其 beam 较大）

### 采样（sampling）

- ✅ 多样性强，更像人话、创作
- ✅ 对话/创意生成常用
- ❌ 不稳定，同一句多次结果不同
- ❌ 可能跑偏

常见经验：

- **翻译/摘要**：beam 4~8
- **对话/创作**：top-p + temperature（而不是 beam）

------

## 8) 束搜索里最常见的“防重复”技巧

1. **repetition_penalty**：重复 token 的 logits 降权
2. **no_repeat_ngram_size**：禁止出现重复 n-gram（比如 3-gram）
3. **coverage/多样性惩罚（diverse beam search）**：鼓励不同 beam 走不同路

在 HF `generate()` 里最常用的是 `no_repeat_ngram_size`。

------

## 9) 复杂度直觉（为什么 beam 不能太大）

每一步：

- 你要跑 K 条序列的 decoder
- 还要做 topk、排序

大致复杂度 ~ (O(K \times T \times \text{模型开销}))

所以：

- beam 越大越慢
- 通常 4、5、8 是比较常见的折中
- 16 往往已经很慢，收益也未必大

------

## 10) 在 HuggingFace 里怎么用（你最可能用到）

```python
gen = model.generate(
    input_ids,
    attention_mask=mask,
    num_beams=4,               # beam size
    length_penalty=1.0,        # 长度惩罚
    early_stopping=True,       # 早停
    no_repeat_ngram_size=3,    # 防重复
    max_new_tokens=64
)
```

------

## 你现在做的是 BART 吗？

BART 做摘要/翻译，一份“经验参数表”：

- `num_beams` 选几
- `length_penalty` 大概多少
- `no_repeat_ngram_size` 设多少最稳
- 以及为什么某些设置会导致“重复/变短/变长”

