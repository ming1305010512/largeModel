

[TOC]

## 1. 优势函数到底要估计什么？

策略梯度里最核心的是：
$$
\nabla_\theta J(\theta)=\mathbb E\left[\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t)\; A^\pi(s_t,a_t)\right]
$$
所以你需要一个 **优势估计** $\hat A_t$，它告诉你：

- 在状态 $s_t$ 下采取动作 $a_t$ 的好坏
- 相对于“平均水平”（baseline）好多少

理论优势：
$$
A^\pi(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)
$$
但 $Q^\pi$ 不好直接估，所以用回报/TD误差来估计。

------

## 2. 先从 1-step TD 误差 $\delta_t$ 开始

定义 TD 残差（TD error）：
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$
它表示：**“我对 $V(s_t)$ 的估计，与从下一步 bootstrap 回来的目标差了多少”**。

------

## 3. 多步优势：n-step advantage 是 $\delta$ 的折扣和

你前面已经推到关键恒等式：
$$
A_t^{(k)} = \sum_{l=0}^{k-1}\gamma^l \delta_{t+l}
$$
含义是：

- $k=1$：只看一步误差（方差小，但偏差大）
- $k$ 越大：越接近 Monte Carlo（偏差小，但方差大）

所以问题来了：**到底选多大的 $k$？**

------

## 4. GAE 的核心：把所有 k-step 优势做指数加权平均

GAE 给出一个“在所有步数之间折中”的定义：
$$
A_t^{GAE(\gamma,\lambda)} = (1-\lambda)\sum_{k=1}^{\infty}\lambda^{k-1} A_t^{(k)}
$$
解释一下每一部分：

- $A_t^{(k)}$：k-step advantage（多步优势）
- $\lambda^{k-1}$：权重随 $k$ 增大指数衰减
- $1-\lambda$：归一化因子（让权重和为1）

把 $A_t^{(k)} = \sum_{l=0}^{k-1}\gamma^l \delta_{t+l}$ 代进去，你会得到更常用的形式：
$$
\boxed{
A_t^{GAE(\gamma,\lambda)}
=\sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l}
}
$$
这就是 GAE 最常用的计算式。

------

## 5. $\gamma$ 和 $\lambda$ 各自控制什么？

### $\gamma$：环境折扣（问题本身）

- 越小：更重视短期奖励
- 越大：更重视长期回报

### $\lambda$：估计折扣（算法选择）

- 控制“用多少步的未来信息来估优势”
- 本质是 **偏差-方差折中旋钮**

直觉：

- $\lambda=0$：只用一步 TD
  $$
  A_t^{GAE}=\delta_t
  $$
  ✅ 方差小，但偏差大（更依赖 $V$ 的准确性）

- $\lambda\to 1$：接近 Monte Carlo advantage
  $$
  A_t^{GAE}\approx G_t - V(s_t)
  $$
  ✅ 偏差小，但方差大（回报噪声更大）

> 所以 $\lambda$ 不是“环境里的折扣”，而是“你愿意相信 bootstrap 多久”的程度。

------

## 6. 为什么 GAE 能写成递推式？（实现时用这个）

从
$$
A_t^{GAE}=\delta_t+(\gamma\lambda)\delta_{t+1}+(\gamma\lambda)^2\delta_{t+2}+\cdots
$$
你可以一眼看出：
$$
\boxed{
A_t^{GAE}=\delta_t+\gamma\lambda A_{t+1}^{GAE}
}
$$
这就是 PPO/ACKTR/TRPO 里最常用的计算方式：**从后往前递推**。

------

## 7. 有终止状态（done）怎么办？

在 episode 结束时，后续项应该被截断，所以递推写成：
$$
A_t = \delta_t + \gamma\lambda(1-d_t)A_{t+1}
$$
其中 $d_t=1$ 表示这一步之后 episode 结束。

同时 TD 误差也要考虑 done：
$$
\delta_t = r_t + \gamma(1-d_t)V(s_{t+1}) - V(s_t)
$$

------

## 8. GAE Target 为什么是 $A_t + V(s_t)$？

你训练 critic 时，需要一个“回报目标” $\hat V_t$，而优势的定义就是：
$$
A_t \approx \hat G_t - V(s_t)
\Rightarrow \hat G_t \approx A_t + V(s_t)
$$
所以 PPO 里常用：
$$
\boxed{
\text{value\_target}_t = A_t^{GAE} + V(s_t)
}
$$
这不是 $\lambda=1$ 才成立，而是“你用什么优势估计，就对应什么回报估计”：

- 当 $\lambda=1$，它更像 Monte Carlo return
- 当 $\lambda<1$，它就是“混合 bootstrap 的 return 估计”

------

## 9. 一段最标准的 GAE 伪代码（PPO 就是这么算）

给你最核心的（反向）：

```
# inputs: rewards r_t, values V(s_t), dones d_t
# output: advantages A_t, value_targets

gae = 0
advantages = []
for t in reversed(range(T)):
    next_value = values[t+1] if t < T-1 else last_value
    delta = rewards[t] + gamma * (1 - dones[t]) * next_value - values[t]
    gae = delta + gamma * lam * (1 - dones[t]) * gae
    advantages.insert(0, gae)

value_targets = advantages + values[:-1]  # elementwise
```

（实际 PPO 还会对 advantages 做 normalize，提升训练稳定性）

------

## 10. 你要记住的“GAE一句话”

> **GAE = 把未来每一步 TD 误差 $\delta$ 按 $(\gamma\lambda)^l$ 折扣求和。
>  $\lambda$ 控制偏差-方差折中：$\lambda=0$ 像 TD，$\lambda\to1$ 像 Monte Carlo。**