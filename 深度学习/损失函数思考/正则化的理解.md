[TOC]

# 本质

## 正则化的本质作用是：

> **在优化过程中对模型“施加限制”，避免模型学得过度、过拟合训练数据，使其泛化能力更强。**

损失函数：
$$
L_{\text{total}} = L_{\text{task}} + \lambda \cdot \|\theta\|^2
$$

## 正则项的本质含义

### 原始任务：

- $L_{\text{task}}$：模型的预测损失（如交叉熵、均方误差）
- 模型希望：损失越小越好 → 拟合训练集越贴合越好

### 正则化之后：

- 增加了 $\lambda \cdot \|\theta\|^2$：**对参数过大惩罚**
- 模型不再单纯追求“拟合训练集”，还要保持 **权重整体尽量小**

### 本质就是：

> 在优化时，**让模型在“拟合数据”和“参数简洁”之间做权衡**



# 角度：损失函数最小目标

## 正则化损失函数：

$$
L_{\text{total}} = L_{\text{task}} + \lambda \cdot \|\theta\|^2
$$

这个目标函数有两个“心愿”：

------

### 第一个心愿：拟合训练数据好（来自 $L_{\text{task}}$）

比如交叉熵、MSE、NLL，目标是：

> “只要预测准，loss 越低越好！”

这个部分**只关注模型在训练集上的表现**，如果它单独存在，它会让模型：

- 参数尽可能往有利于拟合的方向调
- 哪怕参数变得很大，只要能让训练 loss 更小，它就愿意

------

### 但这样做有个隐患：

- 模型会非常**偏执地拟合训练数据**
- 包括**异常值、随机噪声、数据偏差**
- 就像你把一个拟合曲线任务交给了一个“疯狂艺术家”，他画出了非常弯曲、复杂的图像，训练集拟合极好，但**一点泛化能力都没有**。

------

## 所以加入正则化项后：

我们对损失函数说：

> “嘿！我不只关心你是否拟合得好，我还关心你是不是**学得太复杂了**。”

于是加上：
$$
\lambda \cdot \|\theta\|^2
$$
含义是：

- 如果模型用了**很大的参数（代表复杂的结构）**
- 就会被**惩罚 → 总损失变大！**

------

### 损失函数优化目标就变成了两难：

$$
\min_\theta \left[ L_{\text{task}} + \lambda \cdot \|\theta\|^2 \right]
$$

也就是说：

| 如果你拟合得很好但参数太大 → 惩罚！总 loss 高 |
| --------------------------------------------- |
| 如果你参数很小但拟合很差 → 任务 loss 高       |
| 最优状态是：**拟合还不错，参数也不大**        |

这就形成了一个自动平衡机制：

> **不要只顾训练 loss 小，要同时让模型简洁。**

## 怎么个惩罚法

有正则时：


$$
\nabla_\theta L_{\text{total}} = \nabla_\theta L_{\text{task}} + \lambda \cdot \nabla_\theta \|\theta\|^2
= \nabla_\theta L_{\text{task}} + \lambda \cdot 2\theta
$$
每次更新参数的时候，多了这段：
$$
- \eta \cdot \lambda \cdot 2\theta
$$
也就是：

> **你当前这个参数 θ 自己乘以一个负数，然后把自己往 0 的方向拉回来一点**

### 举个例子感受“被惩罚”的过程：

假设：

- 当前某个权重 θ = 10（很大了）
- 学习率 η = 0.01，λ = 0.1

那每次更新：

```
theta -= η * (梯度 + 2 * λ * θ)
       = θ - η * dL_task - η * 2λθ
       = θ - η * dL_task - 0.02 * θ
```

即使当前梯度是 0，权重自己也被“往回拉 2%”。

如果 θ 很大（比如 100），那被拉回来就很多。

如果 θ 很小（比如 0.01），那惩罚几乎没有。

# 为什么“让权重小”有助于泛化？

------

## 一、从“模型表达能力”的角度理解

### 大权重 = 高放大倍数 = 高灵敏度

在神经网络中，每一层的输出是：
$$
z = W x + b
$$
其中 W 是权重矩阵。

如果 W 的值很大，会发生什么？

> **模型的输出会对输入的微小扰动异常敏感！**

也就是说：

- 训练集中的点我记住了
- 但测试集只要有一点点不同，输出就完全偏了

这就是所谓的 **高方差模型** → 不稳 → 过拟合 → 泛化差

### 小权重 = 减少神经元“放大能力” = 减少过拟合

让权重小，相当于：

- 把每一层神经元的“放大能力”变弱
- 整个模型变得更“稳重”“保守”
- 就不会对训练集中的小特征“过分学习”，更关注**整体趋势**

这就是：**泛化能力提升**

## 二、从“函数形状”的角度理解

我们来想象一个回归任务：

- 训练点在二维平面
- 你让模型自由发挥，它可能画出一条“非常曲折的拟合曲线”（像蛇一样穿过每个点）

📉 这样的模型虽然训练误差小，但它画的曲线：

- **波动非常大**
- **在没见过的区域极不可靠**

------

### 而“让权重变小”的效果是：

- 会约束这条函数曲线不能“弯来弯去”
- 要用更平滑、更简洁的曲线拟合点 → 不允许“硬拗”

这就是：**模型变“简单”，拟合的是“趋势”，不是“记忆”**

## 三、从“贝叶斯先验”的角度理解（高阶理解）

具体可查看：

[贝叶斯先验理解]:扩展/贝叶斯先验理解正则（未理解）.md

我们把训练神经网络看成一个贝叶斯优化过程：
$$
\text{Posterior} \propto \text{Likelihood} \cdot \text{Prior}
$$
正则项就相当于**加入了一个先验假设**：

- 我相信一个“好的模型”的参数应该是 **接近 0 的**
- 权重越大，越不可信 → 惩罚越重

### L2 正则的隐含先验：

> **参数服从 N(0, σ²) 的正态分布** → 偏好参数靠近 0

也就是说：

> **从概率角度我们也相信“小权重的模型更可信、更容易泛化”**

------

## 总结回顾：为什么小权重 → 泛化好？

| 理解角度       | 小权重的意义                                           |
| -------------- | ------------------------------------------------------ |
| ⚙️ 模型灵敏度   | 减少输出对输入的极度敏感，提升稳定性                   |
| 🎨 函数形状限制 | 减少模型“画奇怪曲线”的自由度，避免过拟合               |
| 📊 贝叶斯先验   | 相信“好模型”的参数分布应该靠近 0（L2 正则 = 高斯先验） |